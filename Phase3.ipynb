{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsVshh_KogLT"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import torch\n",
        "import random\n",
        "import collections\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4WgUKtL68nE"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSh9j4AxzPi5"
      },
      "source": [
        "#**Required Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVbugVEgZypi"
      },
      "source": [
        "def train_test_split(lang1, lang2, ratio=0.8):\n",
        "    instances = len(lang1)\n",
        "    train_size = int(instances * ratio)\n",
        "    train_hin = []\n",
        "    train_eng = []\n",
        "    val_hin = []\n",
        "    val_eng = []\n",
        "    indices = random.sample(range(instances), train_size)\n",
        "    for index in range(instances):\n",
        "        if index in indices:\n",
        "            train_hin.append(lang1[index])\n",
        "            train_eng.append(lang2[index])\n",
        "        else:\n",
        "            val_hin.append(lang1[index])\n",
        "            val_eng.append(lang2[index])\n",
        "    return train_hin, train_eng, val_hin, val_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k67ODL1_osAd"
      },
      "source": [
        "def tokenize(s, lang):\n",
        "    if lang == 'hin':\n",
        "        s = re.sub(r'[^\\s\\u0900-\\u0963\\u0970-\\u097f]+', ' ', s)\n",
        "        s = re.sub(r' +', ' ', s)\n",
        "    elif lang == 'eng':\n",
        "        s = re.sub(r'[^\\s\\u0041-\\u005a\\u0061-\\u007a]+', ' ', s)\n",
        "        s = re.sub(r' +', ' ', s)\n",
        "        s = s.lower()\n",
        "    tokens = s.split(' ')\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnFyrHk_BrzR"
      },
      "source": [
        "**Reference:** https://d2l.ai/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx1zZmKqwGAB"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        counter = count_corpus(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
        "        uniq_tokens += [\n",
        "            token for token, freq in self.token_freqs\n",
        "            if freq >= min_freq and token not in uniq_tokens]\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "def count_corpus(tokens):  \n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kes1qOyNaPHm"
      },
      "source": [
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]  \n",
        "    return line + [padding_token] * (num_steps - len(line)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xz5BIPRC-8D"
      },
      "source": [
        "def build_array(lines, vocab, num_steps):\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab['<eos>']] for l in lines]\n",
        "    array = torch.tensor([\n",
        "        truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
        "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
        "    return array, valid_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsSuDbMaHf2"
      },
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    dataset = data.TensorDataset(*data_arrays)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_nnT7q3PgRo"
      },
      "source": [
        "class EpochLoss:\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwo8aiFjdSkD"
      },
      "source": [
        "def sequence_mask(X, valid_len, value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                        device=X.device)[None, :] < valid_len[:, None]\n",
        "    X[~mask] = value\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdzskQ3fcyoG"
      },
      "source": [
        "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
        "    def forward(self, pred, label, valid_len):\n",
        "        weights = torch.ones_like(label)\n",
        "        weights = sequence_mask(weights, valid_len)\n",
        "        self.reduction = 'none'\n",
        "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
        "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
        "        return weighted_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtJEJ_023RfI"
      },
      "source": [
        "def masked_softmax(X, valid_lens):\n",
        "    shape = X.shape\n",
        "    valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "    X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
        "    return nn.functional.softmax(X.reshape(shape), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "805ZYr7W1pze"
      },
      "source": [
        "###**Bidirectional GRU Encoder** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtTyxFNf44On"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout, **kwargs):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout = dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, *args):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        outputs, hidden = self.rnn(embedded)   \n",
        "        concated = torch.cat((hidden[0:hidden.size(0):2], hidden[1:hidden.size(0):2]), dim=2)\n",
        "        hidden = torch.tanh(self.fc(concated)) \n",
        "        return outputs, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msZDJQfg1521"
      },
      "source": [
        "###**GRU Decoder with Bahdanau Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG-zDMVnZyT6"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
        "        super().__init__()\n",
        "        self.W_k = nn.Linear(2 * key_size, num_hiddens, bias=False)\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
        "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srd6WAWSHTyp"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = BahdanauAttention(hid_dim, hid_dim, hid_dim, dropout)\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + 2 * hid_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dense = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
        "        outputs, hidden = enc_outputs\n",
        "        return [outputs.permute(1, 0, 2), hidden, enc_valid_lens]\n",
        "\n",
        "    def forward(self, output, state):\n",
        "        enc_outputs, hidden, enc_valid_lens = state\n",
        "        output = self.dropout(self.embedding(output)).permute(1, 0, 2)\n",
        "        outputs, self._attention_weights = [], []\n",
        "        for x in output:\n",
        "            query = torch.unsqueeze(hidden[-1], dim=1)\n",
        "            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
        "            out, hidden = self.rnn(x.permute(1, 0, 2), hidden)\n",
        "            outputs.append(out)\n",
        "            self._attention_weights.append(self.attention.attention_weights)\n",
        "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
        "        return outputs.permute(1, 0, 2), [enc_outputs, hidden, enc_valid_lens]\n",
        "\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZgsxft73TGN"
      },
      "source": [
        "###**Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xboD6fN_9IdD"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, *args):\n",
        "        enc_outputs = self.encoder(src, *args)\n",
        "        state = self.decoder.init_state(enc_outputs, *args)\n",
        "        return self.decoder(trg, state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGECZVa73Z3m"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06tV_hKt-17q"
      },
      "source": [
        "def train(model, data_iter, lr, num_epochs, tgt_vocab, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        eloss = EpochLoss(2)\n",
        "        for batch in data_iter:\n",
        "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
        "            sos = torch.tensor([tgt_vocab['<sos>']] * Y.shape[0],\n",
        "                                device=device).reshape(-1, 1)\n",
        "            dec_input = torch.cat([sos, Y[:, :-1]], 1)  \n",
        "            Y_hat, _ = model(X, dec_input, X_valid_len)\n",
        "            l = criterion(Y_hat, Y, Y_valid_len)\n",
        "            l.sum().backward() \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "            num_tokens = Y_valid_len.sum()\n",
        "            optimizer.step()\n",
        "            with torch.no_grad():\n",
        "                eloss.add(l.sum(), num_tokens)\n",
        "        print(epoch + 1,' Loss: ', eloss[0] / eloss[1])\n",
        "    print(f'Final Loss: {eloss[0] / eloss[1]:.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8wZ-MY3fbj"
      },
      "source": [
        "###Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KArLAmJ9423C"
      },
      "source": [
        "def predict_seq2seq(model, input, hin_vocab, eng_vocab, num_steps, device):\n",
        "    model.eval()\n",
        "    str_h = tokenize(input, 'hin')\n",
        "    str_h = list(filter(('').__ne__, str_h))\n",
        "    hin_tokens = hin_vocab[str_h] + [hin_vocab['<eos>']] \n",
        "    enc_valid_len = torch.tensor([len(hin_tokens)], device=device)\n",
        "    hin_tokens = truncate_pad(hin_tokens, num_steps, hin_vocab['<pad>'])\n",
        "    enc_X = torch.unsqueeze(torch.tensor(hin_tokens, dtype=torch.long, device=device), dim=0)\n",
        "    enc_outputs = model.encoder(enc_X, enc_valid_len)\n",
        "    state = model.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    dec_X = torch.unsqueeze(torch.tensor([eng_vocab['<sos>']], dtype=torch.long, device=device), dim=0)\n",
        "    output_seq = []\n",
        "    for _ in range(num_steps):\n",
        "        Y, state = model.decoder(dec_X, state)\n",
        "        dec_X = Y.argmax(dim=2)\n",
        "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
        "        if pred == eng_vocab['<eos>']:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return ' '.join(eng_vocab.to_tokens(output_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYouGUJXzE-m"
      },
      "source": [
        "#**Main** **Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PNxlnwnzxjo"
      },
      "source": [
        "Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_-kIKPby_AB"
      },
      "source": [
        "hindi, english = [], []\n",
        "with open('/content/drive/MyDrive/AssignmentNLP/train/train.csv', 'r') as file:\n",
        "    lines = csv.reader(file)\n",
        "    for line in lines:\n",
        "        hindi.append(line[1])\n",
        "        english.append(line[2])\n",
        "hindi = hindi[1:]\n",
        "english = english[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPc0udT9z10M"
      },
      "source": [
        "Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPpotE7bzA0x"
      },
      "source": [
        "train_hin, train_eng, val_hin, val_eng = train_test_split(hindi, english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y_4Gc3wz5ns"
      },
      "source": [
        "Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM_Bx6CSsJHJ"
      },
      "source": [
        "hin = []\n",
        "eng = []\n",
        "for h, e in zip(train_hin, train_eng):\n",
        "    str_h = tokenize(h, 'hin')\n",
        "    str_h = list(filter(('').__ne__, str_h))\n",
        "    if len(str_h) != 0:\n",
        "        str_e = tokenize(e, 'eng')\n",
        "        str_e = list(filter(('').__ne__, str_e))\n",
        "        hin.append(str_h)\n",
        "        eng.append(str_e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AedOr49vz9kL"
      },
      "source": [
        "Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahqGb0S_z0D1"
      },
      "source": [
        "hin_vocab = Vocab(hin, min_freq=2, reserved_tokens=['<pad>', '<sos>', '<eos>'])\n",
        "eng_vocab = Vocab(eng, min_freq=2, reserved_tokens=['<pad>', '<sos>', '<eos>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkX2YNxF0A8b"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BobLm81690nH"
      },
      "source": [
        "INPUT_DIM = len(hin_vocab)\n",
        "OUTPUT_DIM = len(eng_vocab)\n",
        "ENC_EMB_DIM = 50\n",
        "DEC_EMB_DIM = 50 \n",
        "HID_DIM = 128\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "LR = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "num_steps = 15 \n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQW8lm0R0MIz"
      },
      "source": [
        "Create Tensors and Data iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyNdLihY3aPP"
      },
      "source": [
        "hin_array, hin_valid_len = build_array(hin, hin_vocab, num_steps)\n",
        "eng_array, eng_valid_len = build_array(eng, eng_vocab, num_steps)\n",
        "data_arrays = (hin_array, hin_valid_len, eng_array, eng_valid_len)\n",
        "data_iter = load_array(data_arrays, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZSlrrma0SHm"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myyAXhPeNKCr"
      },
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnP5byX50U7L"
      },
      "source": [
        "Initialize weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YyCewqc-BEu"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD9nQcXC0Ye_"
      },
      "source": [
        "Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDJeBoJN-pR0"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbEGINLv0cRk"
      },
      "source": [
        "Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd3rZ-PQc4c4"
      },
      "source": [
        "criterion = MaskedSoftmaxCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o05qYsmn0flp"
      },
      "source": [
        "Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gr7Caq3pjfG"
      },
      "source": [
        "train(model, data_iter, LR, NUM_EPOCHS, eng_vocab, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72bbxp5T0iKE"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7SQ6BGwDZ5G"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/AssignmentNLP/Trial15/train_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAkDAFFl0kJ_"
      },
      "source": [
        "Predict validation set answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0oGRSN3PLtA"
      },
      "source": [
        "answers = []\n",
        "for l in val_hin:\n",
        "    answers.append(predict_seq2seq(model, l, hin_vocab, eng_vocab, num_steps, device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCOx58j3HS9a"
      },
      "source": [
        "with open('/content/drive/MyDrive/AssignmentNLP/Trial15/val_ans.txt', 'w') as f:\n",
        "    for l in answers[:-1]:\n",
        "        f.write(l +'\\n')\n",
        "    f.write(answers[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT2auLPh0qq0"
      },
      "source": [
        "BLEU and METEOR score on Validation set answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w41CzIm4Xcie"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBq2Jyfssdie"
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('wordnet')\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "\n",
        "#file1 = open(sys.argv[1], 'r')\n",
        "references = val_eng\n",
        "#file2 = open(sys.argv[2], 'r')\n",
        "hypotheses = answers\n",
        "\n",
        "total_num = len(references)\n",
        "total_bleu_scores = 0\n",
        "total_meteor_scores = 0\n",
        "for i in range(total_num):\n",
        "  total_bleu_scores+=sentence_bleu([references[i].split(\" \")], hypotheses[i].split(\" \"))\n",
        "  total_meteor_scores+=single_meteor_score(references[i], hypotheses[i])\n",
        "\n",
        "bleu_result = total_bleu_scores/total_num\n",
        "meteor_result = total_meteor_scores/total_num\n",
        "\n",
        "print(\"bleu score: \",bleu_result)\n",
        "print(\"meteor score: \",meteor_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrVN4880zCo"
      },
      "source": [
        "Load Dev dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTp9uDk4Y7Fm"
      },
      "source": [
        "hindi_st = []\n",
        "with open('/content/drive/MyDrive/AssignmentNLP/Week3/hindistatements.csv', 'r') as file:\n",
        "    lines = csv.reader(file)\n",
        "    for line in lines:\n",
        "        hindi_st.append(line[2])\n",
        "hindi_st = hindi_st[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmeJUs-J04bY"
      },
      "source": [
        "Predict Dev set answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7scnY_2cHV"
      },
      "source": [
        "ans = []\n",
        "for l in hindi_st:\n",
        "    ans.append(predict_seq2seq(model, l, hin_vocab, eng_vocab, num_steps, device, True)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACoemxe85N4H"
      },
      "source": [
        "with open('/content/drive/MyDrive/AssignmentNLP/Trial15/answer.txt', 'w') as f:\n",
        "    for l in ans[:-1]:\n",
        "        f.write(l +'\\n')\n",
        "    f.write(ans[-1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}